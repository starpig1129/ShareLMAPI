model:
  # Base model name, can be changed to your chosen model
  name: "gpt-2"

  # Choose the loading method, options include:
  # - default: Default loading method
  # - bitsandbytes: Use BitsAndBytesConfig for quantization
  # - peft: Use PEFT (e.g., LoRA) for parameter-efficient fine-tuning
  loading_method: "default"

  # Settings for default loading method
  default:
    device: "cuda"  # Loading device, can be "cpu" or "cuda"
    # Other default settings can be added here

  # Settings for 8-bit quantization using BitsAndBytesConfig
  bitsandbytes:
    device: "cuda"
    # Other BitsAndBytesConfig related settings
    quantization_config:
      # Specific quantization configuration parameters
      quant_type: "nf4" 
      load_in_4bit: True
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_use_double_quant: False

  # Settings for using PEFT (such as LoRA)
  peft:
    device: "cuda"
    peft_type: "lora" 
    peft_config:
      # Specific configuration parameters for LoRA
      r: 8
      lora_alpha: 16
      lora_dropout: 0.1
      target_modules: ["q_proj", "v_proj"]
    # If using other PEFT methods, corresponding configurations can be added here

# Model server settings
model_server:
  model_server_url: "http://localhost:5000"