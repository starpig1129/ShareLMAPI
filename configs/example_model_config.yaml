# configs/model_config.yaml

model:
  # 基礎模型名稱，可以根據需要更改為你選擇的模型
  name: "gpt-2"

  # 選擇載入方法，選項包括:
  # - default: 默認載入方式
  # - bitsandbytes: 使用 BitsAndBytesConfig 進行 8 位量化
  # - peft: 使用 PEFT（例如 LoRA）進行參數高效微調
  loading_method: "default"

  # 默認載入方式的設置
  default:
    device: "cuda"  # 載入設備，可選 "cpu" 或 "cuda"
    # 其他默認設置可以在此添加

  # 使用 BitsAndBytesConfig 進行 8 位量化的設置
  bitsandbytes:
    device: "cuda"
    load_in_8bit: true  # 啟用 8 位量化
    # 其他 BitsAndBytesConfig 相關設置
    quantization_config:
      # 具體的量化配置參數
      quant_type: "nf4"  # 量化類型，可選 "nf4", "fp4" 等
      # 根據需要添加更多配置參數

  # 使用 PEFT（如 LoRA）的設置
  peft:
    device: "cuda"
    peft_type: "lora"  # PEFT 類型，例如 "lora"
    peft_config:
      # LoRA 具體的配置參數
      r: 8
      lora_alpha: 16
      lora_dropout: 0.1
      target_modules: ["q_proj", "v_proj"]  # 需要應用 LoRA 的模塊名稱
    # 如果使用其他 PEFT 方法，可以在此添加相應的配置

adaptive_models:
  # 定義自適應模型載入的不同選項
  default: "gpt-2"
  small: "gpt-2-small"
  medium: "gpt-2-medium"
  large: "gpt-2-large"
  # 可以根據需要添加更多模型選項

# API 相關配置
api:
  host: "0.0.0.0"
  port: 8000
  # 其他 API 相關設置
